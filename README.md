# PekiÅŸtirmeli Ã–ÄŸrenme ile SatranÃ§ ğŸâ™Ÿï¸

Bu proje, **Q-Ã–ÄŸrenme (Q-Learning)** algoritmasÄ± kullanÄ±larak geliÅŸtirilmiÅŸ bir satranÃ§ oyunudur. Oyuncu, beyaz taÅŸlarla hamle yaparken, siyah taÅŸlar bir basit yapay zeka tarafÄ±ndan kontrol edilir. EÄŸitim tamamlandÄ±ktan sonra, Pygame tabanlÄ± bir GUI Ã¼zerinden oyunu oynayabilirsiniz.

---

## ğŸ“‹ Ä°Ã§indekiler
- [Kurulum](#kurulum)
- [NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r?](#nasÄ±l-Ã§alÄ±ÅŸtÄ±rÄ±lÄ±r)
- [YapÄ± ve Teknolojiler](#yapÄ±-ve-teknolojiler)
- [Ã–zellikler](#Ã¶zellikler)
- [KatkÄ±](#katkÄ±)
- [Lisans](#lisans)

---

## ğŸ› ï¸ Kurulum

### Gereksinimler
- Python 3.7+
- Gerekli kÃ¼tÃ¼phaneler:
  ```bash
  pip install pygame chess numpy
Resim DosyalarÄ±
chess_pieces adÄ±nda bir klasÃ¶r oluÅŸturun.

AÅŸaÄŸÄ±daki isimlerde satranÃ§ taÅŸÄ± resimlerini bu klasÃ¶re ekleyin:

Beyaz taÅŸlar: white_pawn.png, white_rook.png, white_knight.png, white_bishop.png, white_queen.png, white_king.png

Siyah taÅŸlar: black_pawn.png, black_rook.png, black_knight.png, black_bishop.png, black_queen.png, black_king.png

âš ï¸ Not: Resim dosyalarÄ± bulunamazsa, GUI taÅŸlarÄ± gÃ¶stermeyecektir.

ğŸš€ NasÄ±l Ã‡alÄ±ÅŸtÄ±rÄ±lÄ±r?
Terminali aÃ§Ä±n ve proje dizinine gidin.

AÅŸaÄŸÄ±daki komutu Ã§alÄ±ÅŸtÄ±rÄ±n:

bash
Copy
python rl_chess.py
EÄŸitim sÃ¼reci baÅŸlayacaktÄ±r (5000 episode). EÄŸitim tamamlandÄ±ÄŸÄ±nda GUI otomatik aÃ§Ä±lÄ±r.

GUI Ã¼zerinden:

Beyaz taÅŸlarla hamle yapmak iÃ§in sol tÄ±k kullanÄ±n.

Siyah taÅŸlar otomatik hamle yapar.

## ğŸ§  Q-Learning HakkÄ±nda

### â“ Q-Learning Nedir?
**Q-Learning**, pekiÅŸtirmeli Ã¶ÄŸrenme (reinforcement learning) algoritmalarÄ±ndan biridir. Bir ajanÄ±n, belirli bir durumda en uygun aksiyonu seÃ§meyi Ã¶ÄŸrenmesini saÄŸlar. Temel mantÄ±k, her durum-aksiyon Ã§ifti iÃ§in bir **Q-deÄŸeri** tutmak ve bu deÄŸerleri deneyimlerle gÃ¼ncellemektir. Q-Learning, **deneme-yanÄ±lma** ve **Ã¶dÃ¼l maksimizasyonu** Ã¼zerine kuruludur.

---

### âš™ï¸ Projede Q-Learning NasÄ±l UygulandÄ±?
1. **Durum Temsili**:  
   Tahta pozisyonu, FEN (Forsyth-Edwards Notation) formatÄ±nda temsil edilir. Ã–rnek: `rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR`.

2. **Aksiyon UzayÄ±**:  
   Her durumda geÃ§erli hamleler (`legal_moves`) aksiyon olarak tanÄ±mlanÄ±r. Ã–rneÄŸin, `e2e4` (piyon ilerletme) veya `g1f3` (at hareketi).

3. **Ã–dÃ¼l Fonksiyonu**:  
   - **Materyal Denge**: TaÅŸlarÄ±n deÄŸerleri (piyon=1, vezir=9 vb.) Ã¼zerinden hesaplanÄ±r.  
   - **Pozisyonel Avantaj**: TaÅŸlarÄ±n merkeze yakÄ±nlÄ±ÄŸÄ±na gÃ¶re bonus puan.  
   - **Hamle Kalitesi**: Åah Ã§ekme, taÅŸ alma veya merkeze hamle gibi taktikler ek Ã¶dÃ¼l getirir.  
   - **Oyun Sonu**: Mat durumunda Â±100, beraberlikte 0 Ã¶dÃ¼l.

4. **Temel Parametreler**:  
   - `epsilon`: KeÅŸif (exploration) oranÄ±. BaÅŸlangÄ±Ã§ta yÃ¼ksek (`1.0`), zamanla azalÄ±r (`0.01`).  
   - `alpha`: Ã–ÄŸrenme oranÄ± (`0.1`).  
   - `gamma`: Gelecek Ã¶dÃ¼llerin indirim faktÃ¶rÃ¼ (`0.9`).  

---

### ğŸ“ˆ Q-Learning'in Projedeki RolÃ¼
- **Beyaz TaÅŸlar (Ä°nsan/KullanÄ±cÄ±)**: Q-Learning ajanÄ± tarafÄ±ndan eÄŸitilir.  
- **Siyah TaÅŸlar (AI)**: Basit bir kural tabanlÄ± algoritma (`SimpleBlackPlayer`) ile kontrol edilir.  
- **EÄŸitim**: 5000 episode boyunca Q-tablosu gÃ¼ncellenir. Her episode, bir satranÃ§ oyununu temsil eder.

---

### âš ï¸ SÄ±nÄ±rlamalar ve Ä°yileÅŸtirme AlanlarÄ±
- **Durum UzayÄ± BÃ¼yÃ¼klÃ¼ÄŸÃ¼**: SatranÃ§ta olasÄ± durum sayÄ±sÄ± Ã§ok yÃ¼ksek olduÄŸundan, Q-tablosu verimsiz olabilir.  
  - Ã‡Ã¶zÃ¼m Ã–nerisi: Derin Q-Networks (DQN) veya durum vektÃ¶rleÅŸtirme.  
- **Ã–dÃ¼l DaÄŸÄ±lÄ±mÄ±**: Seyrek Ã¶dÃ¼l problemi (Ã¶rn., mat durumuna kadar Ã¶dÃ¼l alÄ±namamasÄ±).  
  - Ã‡Ã¶zÃ¼m Ã–nerisi: Ara Ã¶dÃ¼lleri artÄ±rmak veya Monte Carlo Tree Search (MCTS) entegrasyonu.

---

### ğŸ“š Kaynaklar
- [Q-Learning: A Beginner's Guide](https://www.geeksforgeeks.org/q-learning-in-python/)  
- [SatranÃ§ ve PekiÅŸtirmeli Ã–ÄŸrenme](https://towardsdatascience.com/reinforcement-learning-for-chess-9658629f002)  

ğŸ§  YapÄ± ve Teknolojiler
Ana BileÅŸenler
ChessEnvironment: Tahta durumunu, Ã¶dÃ¼l hesaplamalarÄ±nÄ± ve oyun kurallarÄ±nÄ± yÃ¶netir.

QLearningAgent: Q-Ã–ÄŸrenme algoritmasÄ±nÄ± uygular.

SimpleBlackPlayer: Siyah taÅŸlar iÃ§in basit bir hamle seÃ§ici.

ChessGUI: Pygame tabanlÄ± kullanÄ±cÄ± arayÃ¼zÃ¼.

KullanÄ±lan KÃ¼tÃ¼phaneler
pygame: Grafik arayÃ¼z iÃ§in.

chess: SatranÃ§ kurallarÄ± ve tahta yÃ¶netimi iÃ§in.

numpy: Pozisyonel Ã¶dÃ¼l hesaplamalarÄ± iÃ§in.

ğŸŒŸ Ã–zellikler
Q-Ã–ÄŸrenme Optimizasyonu: Epsilon decay, materyal denge ve pozisyonel Ã¶dÃ¼llerle Ã¶ÄŸrenme.

Agresif Siyah Oyuncu: TaÅŸ alma ve ÅŸah Ã§ekme davranÄ±ÅŸlarÄ± optimize edilmiÅŸtir.

GerÃ§ek ZamanlÄ± GUI: Hamleler anÄ±nda gÃ¶rselleÅŸtirilir.

Oyun Sonu Tespiti: Mat, pat ve yetersiz materyal durumlarÄ± desteklenir.

ğŸ¤ KatkÄ±
Bu depoyu fork edin.

Yeni bir branch oluÅŸturun:

bash
Copy
git checkout -b yeni-Ã¶zellik
DeÄŸiÅŸikliklerinizi commit edin:

bash
Copy
git commit -m "Yeni Ã¶zellik eklendi"
DeÄŸiÅŸikliklerinizi push edin:

bash
Copy
git push origin yeni-Ã¶zellik
Pull Request oluÅŸturun.

ğŸ“œ Lisans
Bu proje MIT LisansÄ± altÄ±nda lisanslanmÄ±ÅŸtÄ±r. Detaylar iÃ§in LICENSE dosyasÄ±nÄ± inceleyin.
